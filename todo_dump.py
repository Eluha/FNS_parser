import asyncio
import aiohttp
import asyncpg
import json
import logging
import os
from datetime import datetime
from fake_useragent import UserAgent
from dotenv import load_dotenv

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DB_CONFIG = {
    "dsn": f"postgres://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('POSTGRES_HOST')}/{os.getenv('POSTGRES_DB')}",
    "min_size": 5,
    "max_size": 10
    }

BASE_URL = "https://bo.nalog.gov.ru"
MAX_SEARCH_WORKERS = 3       # –ù–µ —Å—Ç–∞–≤—å –º–Ω–æ–≥–æ, –∑–∞–±–∞–Ω—è—Ç –Ω–∞ –ø–æ–∏—Å–∫–µ
MAX_DETAIL_WORKERS = 2      # –î–µ—Ç–∞–ª–µ–π –º–æ–∂–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –±–æ–ª—å—à–µ
headers_template = {
    'Accept': '*/*',
    'Accept-Language': 'ru,en;q=0.9,de;q=0.8',
    'Connection': 'keep-alive',
    'Cookie': '_ym_uid=1691573085528894859; _ym_d=1691573085; _ym_isad=2; disclaimed=true',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin'
}

class Scraper:

    def __init__(self, db_pool):
        self.pool = db_pool
        self.ua = UserAgent()
        # –û—á–µ—Ä–µ–¥—å –¥–ª—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞)
        self.search_queue = asyncio.Queue()
        # –û—á–µ—Ä–µ–¥—å –¥–ª—è –∑–∞–¥–∞—á —Å–±–æ—Ä–∞ –¥–µ—Ç–∞–ª–µ–π (ID –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π)
        self.detail_queue = asyncio.Queue()
        # –°–µ—Ç –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ ID –≤ –ø–∞–º—è—Ç–∏, —á—Ç–æ–±—ã –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –≤ –æ—á–µ—Ä–µ–¥—å –¥—É–±–ª–∏
        self.seen_ids = set()


    def get_headers(self):
        h = headers_template.copy()
        h["User-Agent"] = self.ua.random
        return h
    

    async def fetch_json(self, session, url, params=None):
        """–û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ –∑–∞–ø—Ä–æ—Å–æ–º —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏"""
        for _ in range(3):
            try:
                async with session.get(url, params=params, headers=self.get_headers(), timeout=15) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    else:
                        logger.warning(f"Status {resp.status} for {url}. Retrying...")
                        await asyncio.sleep(2)
            except Exception as e:
                logger.error(f"Network error {url}: {e}")
                await asyncio.sleep(2)
        return {} # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π dict –ø–æ—Å–ª–µ –≤—Å–µ—Ö –Ω–µ—É–¥–∞—á
    

    async def check_if_search_done(self, okved, region, year, page):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã (Resumability)"""
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(
                """SELECT 1 FROM raw.raw_dump_data_search_params 
                   WHERE okved=$1 AND region_name=$2 AND year=$3 AND total_page=$4 LIMIT 1""", 
                okved, region, year, page
            )
            return row is not None
        

    async def save_search_dump(self,  okved, region, page, year, payload):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä–æ–≥–æ JSON –ø–æ–∏—Å–∫–∞ (ELT)"""
        async with self.pool.acquire() as conn:
            await conn.execute(
                """INSERT INTO raw.raw_dump_data (okved, year, page, region_name, payload) 
                   VALUES ($1, $2, $3, $4, $5)""",
                okved, year, page, region, json.dumps(payload)
            )

    async def save_search_params_log(self, okved, year, region, total_pages, total_elements):
        """–§–∏–∫—Å–∞—Ü–∏—è —Ñ–∞–∫—Ç–∞, —á—Ç–æ –º—ã –æ—Ç—Ä–∞–±–æ—Ç–∞–ª–∏ —ç—Ç–æ—Ç –ø–æ–∏—Å–∫"""
        async with self.pool.acquire() as conn:
            await conn.execute(
                """INSERT INTO raw.raw_dump_data_search_params 
                   (okved, year, region_name, total_page, total_elements) 
                   VALUES ($1, $2, $3, $4, $5) 
                   ON CONFLICT DO NOTHING""",
                okved, year, region, total_pages, total_elements
            )


    async def save_details_dump(self, org_id, info_payload, bfo_payload):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π. –¢—Ä–∏–≥–≥–µ—Ä—ã –±–∞–∑—ã —Å–∞–º–∏ —Ä–∞–∑–±–µ—Ä—É—Ç JSONB"""
        async with self.pool.acquire() as conn:
            async with conn.transaction():
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ (—Ç—Ä–∏–≥–≥–µ—Ä split_org_info —Å—Ä–∞–±–æ—Ç–∞–µ—Ç —Ç—É—Ç)
                await conn.execute(
                    """INSERT INTO raw.raw_organization_dump_data (id, payload) 
                       VALUES ($1, $2) ON CONFLICT (id) DO UPDATE SET last_date = NOW(), payload = EXCLUDED.payload""",
                    org_id, json.dumps(info_payload)
                )
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ë–§–û (—ç—Ç–æ –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –¥–∞–º–ø, —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –Ω–∞ –Ω–µ–≥–æ —É —Ç–µ–±—è –≤—Ä–æ–¥–µ –Ω–µ—Ç –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π)
                await conn.execute(
                    """INSERT INTO raw.raw_organization_bfo_dump_data (id, payload) 
                       VALUES ($1, $2) ON CONFLICT (id) DO UPDATE SET last_date = NOW(), payload = EXCLUDED.payload""",
                    org_id, json.dumps(bfo_payload)
                )

# --- WORKER: –ü–æ–∏—Å–∫ ---
    async def search_worker(self, session):
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–≥–∏–æ–Ω—ã –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –≤–æ—Ä–∫–µ—Ä–∞
        async with self.pool.acquire() as conn:
            # fetch –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π, –±–µ—Ä–µ–º –∏—Ö —Å—Ä–∞–∑—É
            rows = await conn.fetch("""SELECT "name" FROM raw.russian_federal_subjects""")
            list_regions = [row['name'] for row in rows]

        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ª–µ—Ç
        context_data = await self.fetch_json(session, f"{BASE_URL}/nbo/context")
        # –ó–∞—â–∏—Ç–∞, –µ—Å–ª–∏ periods –Ω–µ—Ç –≤ –æ—Ç–≤–µ—Ç–µ
        list_years = context_data.get('bfoPeriods', []) if context_data else []
        
        while True:
            # –í –æ—á–µ—Ä–µ–¥–µ —Ç–æ–ª—å–∫–æ –û–ö–í–≠–î, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ —Å—Ç–∞—Ä—Ç–æ–≤–∞—è —Ç–æ—á–∫–∞
            okved = await self.search_queue.get()
            
            try:
                logger.info(f"üîç Start processing OKVED: {okved}")
                
                # –£–†–û–í–ï–ù–¨ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å—Ç—ã–π –û–ö–í–≠–î
                first_page = await self.fetch_json(session, f"{BASE_URL}/advanced-search/organizations", 
                                                    {"okved": okved, "page": 0, "size": 100})
                
                total_pages = first_page.get("totalPages", 0)
                total_elements = first_page.get("totalElements", 0)
                
                # –°—Ü–µ–Ω–∞—Ä–∏–π –ê: –ú–∞–ª–æ –∑–∞–ø–∏—Å–µ–π, –∫–∞—á–∞–µ–º –≤—Å—ë —Å—Ä–∞–∑—É
                if total_pages <= 100:
                    await self.process_all_pages(session, okved, None, None, first_page, total_pages)
                    # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—Ö –¥–ª—è –û–ö–í–≠–î–ê —Ü–µ–ª–∏–∫–æ–º
                    await self.save_search_params_log(okved, None, None, total_pages, total_elements)
                
                # –°—Ü–µ–Ω–∞—Ä–∏–π –ë: –ú–Ω–æ–≥–æ –∑–∞–ø–∏—Å–µ–π, –∏–¥–µ–º –ø–æ –†–ï–ì–ò–û–ù–ê–ú
                else:
                    logger.info(f"‚ö†Ô∏è OKVED {okved} has {total_elements} elements. Drilling down to Regions.")
                    
                    for region_name in list_regions:
                        # –£–†–û–í–ï–ù–¨ 2: –û–ö–í–≠–î + –†–µ–≥–∏–æ–Ω
                        reg_page = await self.fetch_json(session, f"{BASE_URL}/advanced-search/organizations", 
                                                        {"okved": okved, "address": region_name, "page": 0, "size": 100})
                        
                        reg_total_pages = reg_page.get("totalPages", 0)
                        reg_total_elements = reg_page.get("totalElements", 0)

                        if reg_total_pages == 0:
                            continue

                        # –ï—Å–ª–∏ –≤ —Ä–µ–≥–∏–æ–Ω–µ –º–∞–ª–æ –∑–∞–ø–∏—Å–µ–π - –∫–∞—á–∞–µ–º
                        if reg_total_pages <= 100:
                            await self.process_all_pages(session, okved, region_name, None, reg_page, reg_total_pages)
                            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—Ö –¥–ª—è –ü–ê–†–´ (–û–ö–í–≠–î + –†–ï–ì–ò–û–ù)
                            await self.save_search_params_log(okved, None, region_name, reg_total_pages, reg_total_elements)
                        
                        # –°—Ü–µ–Ω–∞—Ä–∏–π –í: –í —Ä–µ–≥–∏–æ–Ω–µ –¥–æ—Ñ–∏–≥–∞ –∑–∞–ø–∏—Å–µ–π (–ú–æ—Å–∫–≤–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä), –∏–¥–µ–º –ø–æ –ì–û–î–ê–ú
                        else:
                            for year in list_years:
                                # –£–†–û–í–ï–ù–¨ 3: –û–ö–í–≠–î + –†–µ–≥–∏–æ–Ω + –ì–æ–¥
                                year_page = await self.fetch_json(session, f"{BASE_URL}/advanced-search/organizations", 
                                                                    {"okved": okved, "address": region_name, "period": year, "page": 0, "size": 100})
                                
                                year_total_pages = year_page.get("totalPages", 0)
                                year_total_elements = year_page.get("totalElements", 0)
                                
                                if year_total_pages > 0:
                                    # –¢—É—Ç —É–∂–µ –∫–∞—á–∞–µ–º —Å–∫–æ–ª—å–∫–æ –µ—Å—Ç—å, –¥–∞–∂–µ –µ—Å–ª–∏ –±–æ–ª—å—à–µ 100 (–Ω–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ API –æ—Ç–¥–∞—Å—Ç —Ç–æ–ª—å–∫–æ 100)
                                    # –õ–∏–±–æ –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å limit=100
                                    await self.process_all_pages(session, okved, region_name, year, year_page, year_total_pages)
                                    
                                    # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—Ö –¥–ª—è –¢–†–û–ô–ö–ò (–û–ö–í–≠–î + –†–ï–ì–ò–û–ù + –ì–û–î)
                                    await self.save_search_params_log(okved, year, region_name, year_total_pages, year_total_elements)
                                
                                await asyncio.sleep(0.2) # –ú–∏–∫—Ä–æ-–ø–∞—É–∑–∞
            
            except Exception as e:
                logger.error(f"Error in search worker ({okved}): {e}", exc_info=True)
            finally:
                self.search_queue.task_done()

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥, —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏–ø–∞—Å—Ç–∏—Ç—å —Ü–∏–∫–ª—ã
    async def process_all_pages(self, session, okved, region, year, first_page_data, total_pages):
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 0-—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        await self.process_search_page(first_page_data, okved, region=region, year=year, page=0)
        
        # API —á–∞—Å—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–¥–∞—á—É 100 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ (1000 –∑–∞–ø–∏—Å–µ–π)
        limit = min(total_pages, 100)
        
        for page in range(1, limit):
            params = {"okved": okved, "page": page, "size": 100}
            if region: params["address"] = region
            if year: params["period"] = year
            
            data = await self.fetch_json(session, f"{BASE_URL}/advanced-search/organizations", params)
            await self.process_search_page(data, okved, region=region, year=year, page=page)
            await asyncio.sleep(0.5)

    async def process_search_page(self, data, okved, region=None, year=None, page=0):
        if not data or 'content' not in data:
            return
            
        # ELT: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π –ø–æ–∏—Å–∫
        await self.save_search_dump(okved, region, page, year, data)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º ID –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        for item in data['content']:
            org_id = item.get('id')
            if org_id and org_id not in self.seen_ids:
                self.seen_ids.add(org_id)

                try:
                    self.detail_queue.put_nowait(org_id)
                except asyncio.QueueFull:
                    logger.warning(f"Detail queue full, waiting for space...")
                    await self.detail_queue.put(org_id)



    # --- WORKER: –î–µ—Ç–∞–ª–∏ ---
    async def detail_worker(self, session):
        while True:
            org_id = await self.detail_queue.get()
            try:
                # !!! –¢–£–¢ –ü–ê–†–ê–õ–õ–ï–õ–ò–ó–ú !!!
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±–∞ –∑–∞–ø—Ä–æ—Å–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
                url_info = f"{BASE_URL}/nbo/organizations/{org_id}"
                url_bfo = f"{BASE_URL}/nbo/organizations/{org_id}/bfo/"

                # asyncio.gather –∂–¥–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±–æ–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                # return_exceptions=True, —á—Ç–æ–±—ã –æ—à–∏–±–∫–∞ –≤ –æ–¥–Ω–æ–º –Ω–µ –∫—Ä–∞—à–∏–ª–∞ –¥—Ä—É–≥–æ–π (—Ö–æ—Ç—è —Ç—É—Ç –Ω–∞–º –Ω—É–∂–Ω—ã –æ–±–∞)
                res_info, res_bfo = await asyncio.gather(
                    self.fetch_json(session, url_info),
                    self.fetch_json(session, url_bfo)
                )

                if isinstance(res_info, Exception):
                    logger.error(f"Error fetching info for {org_id}: {res_info}")
                    res_info = {}
                    
                if isinstance(res_bfo, Exception):
                    logger.error(f"Error fetching BFO for {org_id}: {res_bfo}")
                    res_bfo = {}

                if res_info:
                    await self.save_details_dump(org_id, res_info, res_bfo)
                else:
                    logger.warning(f"No info data for {org_id}, skipping save")
            except Exception as e:
                logger.error(f"Error detail worker {org_id}: {e}")
            finally:
                self.detail_queue.task_done()

    async def producer(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        for i in range(1, 100): 
            for j in range(1, 100):
                okved = f"{i:02d}.{j:02d}"
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∫–ª–∞–¥–µ–º –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫—É, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–∫–æ–±–æ–∫
                await self.search_queue.put(okved)

async def main():
    pool = None
    try:
        # –°–æ–∑–¥–∞–µ–º –ø—É–ª –ë–î –ü–ï–†–ï–î —Å–µ—Å—Å–∏–µ–π
        pool = await asyncpg.create_pool(
            DB_CONFIG["dsn"], 
            min_size=DB_CONFIG["min_size"], 
            max_size=DB_CONFIG["max_size"]
        )
        
        async with aiohttp.ClientSession() as session:
            scraper = Scraper(pool)

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ—Ä–∫–µ—Ä—ã
            search_tasks = [
                asyncio.create_task(scraper.search_worker(session)) 
                for _ in range(MAX_SEARCH_WORKERS)
            ]
            detail_tasks = [
                asyncio.create_task(scraper.detail_worker(session)) 
                for _ in range(MAX_DETAIL_WORKERS)
            ]

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–¥–∞—á
            await scraper.producer()

            # –ñ–¥–µ–º, –ø–æ–∫–∞ –æ—á–µ—Ä–µ–¥—å –ø–æ–∏—Å–∫–∞ –æ–ø—É—Å—Ç–µ–µ—Ç
            logger.info("Waiting for search queue to finish...")
            await scraper.search_queue.join()
            
            # –ñ–¥–µ–º, –ø–æ–∫–∞ –æ—á–µ—Ä–µ–¥—å –¥–µ—Ç–∞–ª–µ–π –æ–ø—É—Å—Ç–µ–µ—Ç
            logger.info("Waiting for detail queue to finish...")
            await scraper.detail_queue.join()

            # –û—Ç–º–µ–Ω—è–µ–º –≤–æ—Ä–∫–µ—Ä—ã (–æ–Ω–∏ –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ)
            for task in search_tasks + detail_tasks:
                task.cancel()
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
            try:
                await asyncio.gather(*search_tasks, *detail_tasks, return_exceptions=True)
            except Exception as e:
                logger.error(f"Error during task cancellation: {e}")
    
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—É–ª –≤ finally –±–ª–æ–∫–µ
        if pool:
            await pool.close()
            logger.info("Database pool closed.")
        logger.info("Done.")

# –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—É–ª —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        await pool.close()
        logger.info("Done.")

if __name__ == "__main__":
    # –î–ª—è Windows –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è:
    # asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())